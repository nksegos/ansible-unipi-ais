---

- name: Load dependent role vars
  include_vars:
    dir: "{{ role_path }}/../{{ dependent_role }}/vars/"
  loop:
    - 'database'
    - 'consumer'
    - 'producer'
    - 'kafka'
  loop_control:
    loop_var: 'dependent_role'

- name: Truncate {{ ais_static_table }} and {{ ais_kinematic_table }} on {{ database_host }}
  command:
    cmd: |
      /usr/bin/psql -d {{ dbname }}_bench -c 'TRUNCATE TABLE {{ ais_static_table }}; TRUNCATE TABLE {{ ais_kinematic_table }};'
  become_user: 'postgres'
  delegate_to: "{{ database_host }}"

- name: Destroy and recreate the Kafka topics prior to starting the benchmark
  block:

    - name: Stop Kafka Connect service
      systemd_service:
        name: "kafka-connect"
        state: stopped
      delegate_to: "{{ kafka_broker_host }}"

    - name: Delete existing Kafka topics
      shell:
        cmd: |
          {{ kafka_main }}/bin/kafka-topics.sh --bootstrap-server {{ kafka_broker_host }}:{{ kafka_broker_port }} --delete --if-exists --topic {{ kafka_topics | select('match', '.*static.*') | first }}_bench ;
          {{ kafka_main }}/bin/kafka-topics.sh --bootstrap-server {{ kafka_broker_host }}:{{ kafka_broker_port }} --delete --if-exists --topic {{ kafka_topics | select('match', '.*kinematic.*') | first }}_bench ;
      delegate_to: "{{ kafka_broker_host }}"

    - name: Start Kafka Connect service and implicitly re-create benchmark topics
      systemd_service:
        name: "kafka-connect"
        state: started
      delegate_to: "{{ kafka_broker_host }}"

- name: Ensure consumer log is absent
  file:
    path: "/tmp/consumer_{{ dataset_id }}_Kafka_bench.csv"
    state: absent
  delegate_to: "{{ consumer_host }}"

- name: Ensure producer log is absent
  file:
    path: "/tmp/producer_{{ dataset_id }}_Kafka_bench.csv"
    state: absent
  delegate_to: "{{ producer_host }}"

- name: Start Kafka consumer client
  command:
    cmd: |
      {{ consumer_base }}/venv/bin/python3 {{ consumer_base }}/bin/kafka-consumer-bench.py -o /tmp/consumer_{{ dataset_id }}_Kafka_bench.csv
  async: 7200
  poll: 0
  register: consumer_task
  delegate_to: "{{ consumer_host }}"

- name: Wait 30 seconds
  pause:
    seconds: 30

- name: Start Kafka producer client
  command:
    cmd: |
      {{ producer_base }}/venv/bin/python3 {{ producer_base }}/bin/kafka-producer-realtime.py -i {{ producer_base }}/data/{{ dataset_id }}_peak_enriched.csv --latency-benchmark-mode -o /tmp/producer_{{ dataset_id}}_Kafka_bench.csv
  async: 7200
  poll: 0
  register: producer_task
  delegate_to: "{{ producer_host }}"

- name: Wait for Kafka consumer client to exit
  async_status:
    jid: "{{ consumer_task.ansible_job_id }}"
  register: async_poll_consumer
  until: async_poll_consumer.finished
  retries: 120
  delay: 60
  delegate_to: "{{ consumer_host }}"

- name: Wait for Kafka producer to exit
  async_status:
    jid: "{{ producer_task.ansible_job_id }}"
  register: async_poll_producer
  until: async_poll_producer.finished
  retries: 120
  delay: 60
  delegate_to: "{{ producer_host }}"

- name: Retrieve consumer log
  fetch:
    src: "/tmp/consumer_{{ dataset_id }}_Kafka_bench.csv"
    dest: "{{ role_path }}/files/benchmark_results/kafka/"
    flat: true
  delegate_to: "{{ consumer_host }}"

- name: Retrieve producer log
  fetch:
    src: "/tmp/producer_{{ dataset_id }}_Kafka_bench.csv"
    dest: "{{ role_path }}/files/benchmark_results/kafka/"
    flat: true
  delegate_to: "{{ producer_host }}"

...
